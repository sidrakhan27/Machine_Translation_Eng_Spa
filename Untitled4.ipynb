{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Let's import all the necessary libraries"
      ],
      "metadata": {
        "id": "p-eEp_DJuh0E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paBGHH36BVJP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "import tensorflow.strings as tf_strings\n",
        "import tensorflow.data as tf_data\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\""
      ],
      "metadata": {
        "id": "cYNEJY-iC2Lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's download an English-to-Spanish Translation dataset."
      ],
      "metadata": {
        "id": "vbHSY49qutlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = keras.utils.get_file(\n",
        "    fname=\"spa-eng.zip\",\n",
        "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
        "    extract=True,\n",
        ")\n",
        "text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UPv1StdBZFh",
        "outputId": "38f8c21a-75d6-4235-872e-5994d61b1dc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2638744/2638744 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When parsing the data, we treat each line as a pair of sentences: an English sentence and its corresponding Spanish sentence. The English sentence serves as the source sequence, while the Spanish sentence is the target sequence. To prepare the Spanish sentence for training, we add a special token \"[start]\" at the beginning and \"[end]\" at the end.\n"
      ],
      "metadata": {
        "id": "YNcaUo5jvGc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(text_file, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Process each line to create text pairs\n",
        "text_pairs = [(line.split(\"\\t\")[0], \"[start] \" + line.split(\"\\t\")[1].strip() + \" [end]\") for line in lines]\n",
        "for _ in range(5):\n",
        "    print(random.choice(text_pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugd7y5pYxb3S",
        "outputId": "41e97957-d870-4bd0-c597-3b9ec68b1c91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\"I'm not bossy.\", '[start] No soy mandón. [end]')\n",
            "('Many people use ATMs to withdraw money.', '[start] Mucha gente usa cajeros automáticos para retirar dinero. [end]')\n",
            "('Tom became a successful photographer.', '[start] Tom se convirtió en un exitoso fotógrafo. [end]')\n",
            "('Cows provide us with milk.', '[start] Las vacas nos proveen de leche. [end]')\n",
            "(\"It's time to go.\", '[start] Es hora de irse. [end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is what our sentence looks like"
      ],
      "metadata": {
        "id": "DEFjI7RlvMGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's split the dataset into training and testing part"
      ],
      "metadata": {
        "id": "FvrvuhWBvSWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(text_pairs)\n",
        "\n",
        "# Define the sizes of the validation and test sets\n",
        "val_size = int(0.15 * len(text_pairs))\n",
        "test_size = val_size\n",
        "\n",
        "# Split the text_pairs into training, validation, and test sets\n",
        "train_pairs, remaining_pairs = train_test_split(text_pairs, test_size=val_size * 2, random_state=42)\n",
        "val_pairs, test_pairs = train_test_split(remaining_pairs, test_size=val_size, random_state=42)\n",
        "\n",
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")\n",
        "print(f\"{len(test_pairs)} test pairs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVqzpSvmESQJ",
        "outputId": "5fa6cb06-22f6-4828-fa0a-8e130dc7a27f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "118964 total pairs\n",
            "83276 training pairs\n",
            "17844 validation pairs\n",
            "17844 test pairs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To vectorize the text data, we use two TextVectorization layers: one for English and one for Spanish. These layers convert the original strings into integer sequences, with each integer representing the index of a word in a vocabulary.\n",
        "\n",
        "The English layer standardizes the text by stripping punctuation characters and splitting on whitespace. For the Spanish layer, we add the character \"¿\" to the set of punctuation characters to be stripped.\n",
        "\n",
        "Note: In a production-grade machine translation model, it's recommended to avoid stripping punctuation characters. Instead, each punctuation character could be turned into its own token by providing a custom split function to the TextVectorization layer.\n"
      ],
      "metadata": {
        "id": "UnOlHYOTvorw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n"
      ],
      "metadata": {
        "id": "AtJ0SLgJFvXv"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "batch_size = 64"
      ],
      "metadata": {
        "id": "gOgSZiuXFvax"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    strip_chars = \"¿\"  # Add the character \"¿\" to the set of punctuation characters to be stripped\n",
        "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        "\n",
        "eng_vectorization = TextVectorization(max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length)\n",
        "spa_vectorization = TextVectorization(max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length + 1, standardize=custom_standardization)\n",
        "train_eng_texts = [pair[0] for pair in train_pairs]\n",
        "train_spa_texts = [pair[1] for pair in train_pairs]\n",
        "eng_vectorization.adapt(train_eng_texts)\n",
        "spa_vectorization.adapt(train_spa_texts)"
      ],
      "metadata": {
        "id": "Ta5HxCiVFvgp"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wKjZBdFkd0ph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For dataset formatting, each training step predicts target words N+1 using the source sentence and target words 0 to N. The training dataset yields tuples of encoder_inputs (vectorized source) and decoder_inputs (target words 0 to N) along with targets (next words in the target sentence)."
      ],
      "metadata": {
        "id": "oKC4-JOuwCNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_dataset(eng, spa):\n",
        "    eng = eng_vectorization(eng)\n",
        "    spa = spa_vectorization(spa)\n",
        "    return (\n",
        "        {\n",
        "            \"encoder_inputs\": eng,\n",
        "            \"decoder_inputs\": spa[:, :-1],\n",
        "        },\n",
        "        spa[:, 1:],\n",
        "    )\n",
        "\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, spa_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    spa_texts = list(spa_texts)\n",
        "    dataset = tf_data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset)\n",
        "    return dataset.cache().shuffle(2048).prefetch(16)\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "metadata": {
        "id": "Qw65MBhXFvio"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the sequence shape (we have batches of 64 pairs, and all sequences are 20 steps long)"
      ],
      "metadata": {
        "id": "q8PaDQkOwRvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
        "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPO3A0AYFvlK",
        "outputId": "c78fe396-cc85-4e60-e65d-91522f1fd9ce"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs[\"encoder_inputs\"].shape: (64, 20)\n",
            "inputs[\"decoder_inputs\"].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs[\"decoder_inputs\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zR7EqoF9T028",
        "outputId": "c0e62371-2f87-44f6-9fb8-70769af9cb00"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([ 2 16 26  1  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0], shape=(20,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs[\"encoder_inputs\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Se3mSWGbfBpC",
        "outputId": "fda67ed5-3b5b-42a3-ad54-e3d4d005632a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[  24   23    5 1314   20    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0], shape=(20,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our sequence-to-sequence Transformer comprises a TransformerEncoder and TransformerDecoder. The TransformerEncoder processes the source sequence to generate a new representation, which is then passed to the TransformerDecoder along with the target sequence up to the current point. The TransformerDecoder predicts the next words in the target sequence, ensuring it only uses information from past tokens via causal masking to prevent future information leakage."
      ],
      "metadata": {
        "id": "JcQNIecUw0x-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(dense_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, None, :], dtype=\"int32\")\n",
        "        else:\n",
        "            padding_mask = None\n",
        "\n",
        "        attention_output = self.attention(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
        "        )\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"embed_dim\": self.embed_dim,\n",
        "                \"dense_dim\": self.dense_dim,\n",
        "                \"num_heads\": self.num_heads,\n",
        "            }\n",
        "        )\n",
        "        return config\n"
      ],
      "metadata": {
        "id": "Knep7nqcQWNB"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        if mask is None:\n",
        "            return None\n",
        "        else:\n",
        "            return tf.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"sequence_length\": self.sequence_length,\n",
        "                \"vocab_size\": self.vocab_size,\n",
        "                \"embed_dim\": self.embed_dim,\n",
        "            }\n",
        "        )\n",
        "        return config\n"
      ],
      "metadata": {
        "id": "LE8H2m63Yyz7"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(latent_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, None, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        else:\n",
        "            padding_mask = None\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
        "        )\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(out_2)\n",
        "        return self.layernorm_3(out_2 + proj_output)\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, None]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, sequence_length, sequence_length))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.convert_to_tensor([1, 1])],\n",
        "            axis=0,\n",
        "        )\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"embed_dim\": self.embed_dim,\n",
        "                \"latent_dim\": self.latent_dim,\n",
        "                \"num_heads\": self.num_heads,\n",
        "            }\n",
        "        )\n",
        "        return config"
      ],
      "metadata": {
        "id": "S0KzS2H6Yy37"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 256\n",
        "latent_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "transformer_encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "transformer_decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(transformer_encoder_inputs)\n",
        "transformer_encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(transformer_decoder_inputs)\n",
        "decoder_state_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, decoder_state_inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "transformer_decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "\n",
        "transformer_encoder = keras.Model(transformer_encoder_inputs, transformer_encoder_outputs)\n",
        "transformer_decoder = keras.Model([transformer_decoder_inputs, decoder_state_inputs], transformer_decoder_outputs)\n",
        "\n",
        "decoder_outputs = transformer_decoder([transformer_decoder_inputs, transformer_encoder_outputs])\n",
        "transformer = keras.Model(\n",
        "    [transformer_encoder_inputs, transformer_decoder_inputs], decoder_outputs, name=\"transformer\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "O4l0lcKaMvxv"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops import math_ops as ops\n"
      ],
      "metadata": {
        "id": "pephpTYEOX8E"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 3\n",
        "\n",
        "transformer.summary()\n",
        "transformer.compile(\n",
        "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Wf8DHjdMvtD",
        "outputId": "441fa51a-5576-45ab-e7d5-0a62bc9b1a41"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " encoder_inputs (InputLayer  [(None, None)]               0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " positional_embedding_2 (Po  (None, None, 256)            3845120   ['encoder_inputs[0][0]']      \n",
            " sitionalEmbedding)                                                                               \n",
            "                                                                                                  \n",
            " decoder_inputs (InputLayer  [(None, None)]               0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " transformer_encoder_1 (Tra  (None, None, 256)            3155456   ['positional_embedding_2[0][0]\n",
            " nsformerEncoder)                                                   ']                            \n",
            "                                                                                                  \n",
            " model_3 (Functional)        (None, None, 15000)          1295964   ['decoder_inputs[0][0]',      \n",
            "                                                          0          'transformer_encoder_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 19960216 (76.14 MB)\n",
            "Trainable params: 19960216 (76.14 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/3\n",
            "1302/1302 [==============================] - 4053s 3s/step - loss: 1.7496 - accuracy: 0.7499 - val_loss: 1.2186 - val_accuracy: 0.8121\n",
            "Epoch 2/3\n",
            "1302/1302 [==============================] - 4062s 3s/step - loss: 1.1393 - accuracy: 0.8231 - val_loss: 0.9325 - val_accuracy: 0.8459\n",
            "Epoch 3/3\n",
            "1302/1302 [==============================] - 4030s 3s/step - loss: 0.9328 - accuracy: 0.8487 - val_loss: 0.8167 - val_accuracy: 0.8608\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7db95bb19090>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pLFouxbhQEZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XiOYcdIGQEcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tkR4yGUqQEgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0NuwHxt6L_tL"
      }
    }
  ]
}